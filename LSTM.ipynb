{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KPdO5M8fh6mm","colab_type":"text"},"source":["### This script builds a LSTM model to perform sentiment analysis in the area of US airline service using Twitter.\n","\n","### Input: \"train.csv\" and \"test.csv\" generated by \"airline.py\". The reason of doing this is using same train and test datasets with other models. Both of them include Tweets and the user's attitude: negative, neutral or positive.\n","\n","### Output: predictions"]},{"cell_type":"code","metadata":{"id":"IFhTIGWLXTbb","colab_type":"code","outputId":"67dc326b-0996-477c-bca9-de3af88a1eee","executionInfo":{"status":"ok","timestamp":1575573633839,"user_tz":300,"elapsed":1463,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LNREkky3WuMD","colab_type":"code","outputId":"23a54bc4-d6e9-4344-da52-f1c1577e41a1","executionInfo":{"status":"ok","timestamp":1575573635842,"user_tz":300,"elapsed":3439,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer \n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","stopWords = set(stopwords.words('english'))\n","\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from sklearn.utils import resample\n","from sklearn.utils import shuffle\n","\n","np.random.seed(101)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"YXz_6J1-2ifj","colab_type":"text"},"source":["## data pre-processing"]},{"cell_type":"code","metadata":{"id":"zup6uUJL2iJ1","colab_type":"code","colab":{}},"source":["# Lemmatization based on words' POS tags\n","def get_wordnet_pos(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","    \n"," # use Wordnet(lexical database) to lemmatize text \n","def lemmatize_text(text):\n","    \n","    lmtzr = WordNetLemmatizer().lemmatize\n","    text = word_tokenize(str(text))   # Init the Wordnet Lemmatizer    \n","    word_pos = pos_tag(text)    \n","    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n","    return (' '.join(lemm_words))\n","\n","# clean and normalize text\n","def pre_process(text):    \n","    \n","    emoji_pattern = re.compile(\"[\"\n","                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                       u\"\\U00002702-\\U000027B0\"\n","                       u\"\\U000024C2-\\U0001F251\"\n","                       \"]+\", flags=re.UNICODE)    \n","\n","    text = emoji_pattern.sub(r'', text)                       # remove emojis       \n","    text = text.lower()                                       # lowercase all letters   \n","#    text = re.sub(r'@[A-Za-z0-9]+', '', text)                # remove user mentions, e.g. @VirginAmerica    \n","    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)       # remove URL links \n","\n","#    white_list = [\"not\", \"no\", \"won't\", \"isn't\", \"couldn't\", \"wasn't\", \"didn't\", \"shouldn't\", \n","#                  \"hasn't\", \"wouldn't\", \"haven't\", \"weren't\", \"hadn't\", \"shan't\", \"doesn't\",\n","#                  \"mightn't\", \"mustn't\", \"needn't\", \"don't\", \"aren't\", \"won't\"]\n","#    words = text.split()\n","#    text = ' '.join([t for t in words if (t not in stopwords.words('english') or t in white_list)])  # remove stopwords        \n","\n","    text = ''.join([t for t in text if t not in string.punctuation])   # remove all punctuations       \n","    text = ''.join([t for t in text if not t.isdigit()])   # remove all numeric digits     \n","    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)   # letters only         \n","    text = lemmatize_text(text)   # use Wordnet(lexical database) to lemmatize text     \n","#    text = stemmer_text(text)   # stem text \n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVEvWCuLgfBG","colab_type":"text"},"source":["## import training and test datasets generated by \"airline.py\""]},{"cell_type":"code","metadata":{"id":"5vZX6sFjW5JB","colab_type":"code","outputId":"07de6f3d-bc00-4c4c-b617-beae1f711f9b","executionInfo":{"status":"ok","timestamp":1575573654387,"user_tz":300,"elapsed":21957,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":407}},"source":["train_X = pd.read_csv(\"/content/drive/My Drive/train.csv\")\n","train_X['processed_text'] =  train_X['text'].apply(pre_process)  \n","train_X['label_convert'] = train_X['airline_sentiment'].map({'negative':0, 'neutral':1, 'positive': 2})\n","train_Y = np.array(train_X['label_convert'])\n","\n","test_X = pd.read_csv(\"/content/drive/My Drive/test.csv\")\n","test_X['processed_text'] =  test_X['text'].apply(pre_process)  \n","test_X['label_convert'] = test_X['airline_sentiment'].map({'negative':0, 'neutral':1, 'positive': 2})\n","test_Y = np.array(test_X['label_convert'])\n","\n","#Y = pd.get_dummies(data['airline_sentiment']) #.values\n","#data['label_convert'] = data['airline_sentiment'].map({'negative':0, 'neutral':1, 'positive': 2})\n","#Y = np.array(data['label_convert'])\n","\n","#train_X, test_X, train_Y, test_Y = train_test_split(data, Y, test_size = 0.2, random_state=42)\n","\n","print(train_X.shape,train_Y.shape)\n","print(test_X.shape,test_Y.shape)\n","\n","\n","# No oversampling and undersampling\n","trainX_sampled = train_X\n","print(trainX_sampled.shape)\n","\n","train_X[['text', 'processed_text']].head(10)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(11712, 7) (11712,)\n","(2928, 7) (2928,)\n","(11712, 7)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>processed_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@united Hi.  My relative's Flight Booking Prob...</td>\n","      <td>united hi my relative flight book problem numb...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@AmericanAir served the nastiest food Ive ever...</td>\n","      <td>americanair serve the nasty food ive ever see ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@united yes it is partly used, the del-ewr is ...</td>\n","      <td>united yes it be partly use the delewr be use ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@JetBlue Not helping since there's a bunch of ...</td>\n","      <td>jetblue not help since there a bunch of u try ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@united Will never fly with you again! Terribl...</td>\n","      <td>unite will never fly with you again terrible s...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>@USAirways @AmericanAir 2hrs Late Flightr fina...</td>\n","      <td>usairways americanair hrs late flightr finally...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>@JetBlue THANK YOU! I am your new big fan :)</td>\n","      <td>jetblue thank you i be your new big fan</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>@SouthwestAir Seriously? FOUR DELAYS? Only tak...</td>\n","      <td>southwestair seriously four delay only take mi...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>@AmericanAir still waiting for a flight... I s...</td>\n","      <td>americanair still wait for a flight i should g...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>@united we are sitting on the runway for 2 hou...</td>\n","      <td>unite we be sit on the runway for hour it be r...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text                                     processed_text\n","0  @united Hi.  My relative's Flight Booking Prob...  united hi my relative flight book problem numb...\n","1  @AmericanAir served the nastiest food Ive ever...  americanair serve the nasty food ive ever see ...\n","2  @united yes it is partly used, the del-ewr is ...  united yes it be partly use the delewr be use ...\n","3  @JetBlue Not helping since there's a bunch of ...  jetblue not help since there a bunch of u try ...\n","4  @united Will never fly with you again! Terribl...  unite will never fly with you again terrible s...\n","5  @USAirways @AmericanAir 2hrs Late Flightr fina...  usairways americanair hrs late flightr finally...\n","6       @JetBlue THANK YOU! I am your new big fan :)            jetblue thank you i be your new big fan\n","7  @SouthwestAir Seriously? FOUR DELAYS? Only tak...  southwestair seriously four delay only take mi...\n","8  @AmericanAir still waiting for a flight... I s...  americanair still wait for a flight i should g...\n","9  @united we are sitting on the runway for 2 hou...  unite we be sit on the runway for hour it be r..."]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"kHYczJpFmUUD","colab_type":"text"},"source":["# undersample majority class"]},{"cell_type":"code","metadata":{"id":"bPHqiuTMmJff","colab_type":"code","colab":{}},"source":["#df_major_neg = train_X[train_X['label'] == -1]\n","#df_minor_neu = train_X[train_X['label'] == 0]\n","#df_minor_pos = train_X[train_X['label'] == 1]        \n","#minor_count = len(df_minor_pos)\n","\n","#df_major_neg_undersampled = resample(df_major_neg, \n","#                              replace = True,              # sample with replacement\n","#                              n_samples = minor_count,     # to match minority class\n","#                              random_state = 1000)    \n","\n","#df_minor_neu_undersampled = resample(df_minor_neu, \n","#                              replace = True,             \n","#                              n_samples = minor_count,   \n","#                              random_state = 1000)      \n","      \n","#train_sampled = pd.concat([df_major_neg_undersampled, df_minor_neu_undersampled, df_minor_pos])   # Combine majority class with oversampled minority class\n","#print(\"Train dataset calss distribution: \\n\", train_sampled.label.value_counts())\n","#train_sampled = shuffle(train_sampled, random_state = 200) \n","#print(trainX_sampled.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCsFHB7cl7tO","colab_type":"text"},"source":["# oversample minority class"]},{"cell_type":"code","metadata":{"id":"Fv4yA1lHl3_V","colab_type":"code","colab":{}},"source":["#df_minor_neu_oversampled = resample(df_minor_neu, \n","#                              replace = True,              # sample with replacement\n","#                              n_samples = major_count,     # to match majority class \n","#                              random_state = 1000)    \n","\n","#df_minor_pos_oversampled = resample(df_minor_pos, \n","#                              replace = True,             \n","#                              n_samples = major_count,   \n","#                              random_state = 1000)      \n","      \n","#train_sampled = pd.concat([df_major_neg, df_minor_neu_oversampled, df_minor_pos_oversampled])   # Combine majority class with oversampled minority class\n","#print(\"Train dataset calss distribution: \\n\", train_sampled.label.value_counts())\n","#train_sampled = shuffle(train_sampled, random_state = 200) \n","\n","#print(trainX_sampled.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDuMdXghg4Va","colab_type":"text"},"source":["## creates the vocabulary index based on word frequency, takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. "]},{"cell_type":"code","metadata":{"id":"mQerGpm58ut9","colab_type":"code","outputId":"c6d7d688-a2c2-42cc-e247-dc4520095319","executionInfo":{"status":"ok","timestamp":1575573654891,"user_tz":300,"elapsed":22410,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(trainX_sampled['processed_text'])\n","\n","trainX = tokenizer.texts_to_sequences(trainX_sampled['processed_text'].values)\n","testX = tokenizer.texts_to_sequences(test_X['processed_text'].values)\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","print(vocab_size)  \n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["10347\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R4NhJze_hHAS","colab_type":"text"},"source":["## text padding"]},{"cell_type":"code","metadata":{"id":"vKhF396n9ZQx","colab_type":"code","outputId":"a8f60d1f-66c5-46be-90ff-c9e9b64087c5","executionInfo":{"status":"ok","timestamp":1575573654892,"user_tz":300,"elapsed":22383,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":260}},"source":["max_len = 35\n","trainX_pad = pad_sequences(trainX, maxlen = max_len)\n","testX_pad = pad_sequences(testX, maxlen = max_len)\n","print(trainX_pad)\n","print(testX_pad)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[   0    0    0 ... 4379   63    7]\n"," [   0    0    0 ... 4382   33  461]\n"," [   0    0    0 ... 1815   16 1256]\n"," ...\n"," [   0    0    0 ...   30  173  741]\n"," [   0    0    0 ...   16    3  437]\n"," [   0    0    0 ...    0   14   38]]\n","[[   0    0    0 ...  131   17  764]\n"," [   0    0    0 ...  357    8  151]\n"," [   0    0    0 ...   70   17    1]\n"," ...\n"," [   0    0    0 ...    8  110  266]\n"," [   0    0    0 ...   76   30  108]\n"," [   0    0    0 ...  110  144 5223]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cTDhzBa6hKXT","colab_type":"text"},"source":["## build LSTM model"]},{"cell_type":"code","metadata":{"id":"gYc4zMTsWzpv","colab_type":"code","outputId":"60ef19fc-866c-4c6b-df6e-18f96b1bfeea","executionInfo":{"status":"ok","timestamp":1575573655225,"user_tz":300,"elapsed":22695,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":557}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM\n","from keras import metrics\n","from keras import regularizers\n","\n","embed_dim = 16\n","lstm_out = 8\n","def buildModel():     \n","    model = Sequential()\n","    model.add(Embedding(vocab_size, embed_dim, input_length=max_len))\n","    model.add(LSTM(lstm_out, dropout = 0.5))\n","    model.add(Dense(3, activation='softmax')) #, kernel_regularizer=regularizers.l2(0.005)))\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[metrics.sparse_categorical_accuracy])\n","    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model\n","model = buildModel()\n","print(model.summary())"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 35, 16)            165552    \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 8)                 800       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 27        \n","=================================================================\n","Total params: 166,379\n","Trainable params: 166,379\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K7LPpZvKhUUI","colab_type":"text"},"source":["## moded fitting on train data"]},{"cell_type":"code","metadata":{"id":"-5RLfEkRW8Gw","colab_type":"code","outputId":"ecf54e02-aace-4ea2-d0c1-fb358a929e6a","executionInfo":{"status":"ok","timestamp":1575573713064,"user_tz":300,"elapsed":80502,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":661}},"source":["model.fit(trainX_pad,trainX_sampled['label_convert'], epochs = 8, batch_size = 48, verbose = 2)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/8\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"," - 8s - loss: 0.8278 - sparse_categorical_accuracy: 0.6533\n","Epoch 2/8\n"," - 7s - loss: 0.6201 - sparse_categorical_accuracy: 0.7392\n","Epoch 3/8\n"," - 7s - loss: 0.5280 - sparse_categorical_accuracy: 0.7964\n","Epoch 4/8\n"," - 7s - loss: 0.4357 - sparse_categorical_accuracy: 0.8415\n","Epoch 5/8\n"," - 7s - loss: 0.3708 - sparse_categorical_accuracy: 0.8642\n","Epoch 6/8\n"," - 7s - loss: 0.3222 - sparse_categorical_accuracy: 0.8866\n","Epoch 7/8\n"," - 7s - loss: 0.2889 - sparse_categorical_accuracy: 0.8973\n","Epoch 8/8\n"," - 7s - loss: 0.2643 - sparse_categorical_accuracy: 0.9054\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7ff24c85bfd0>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"SJACkev8hhAK","colab_type":"text"},"source":["## get model score and overall accuracy"]},{"cell_type":"code","metadata":{"id":"0YTWK6P5W-_C","colab_type":"code","outputId":"c4cd92d5-4616-485d-c906-66057a57206d","executionInfo":{"status":"ok","timestamp":1575573713460,"user_tz":300,"elapsed":80866,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["score, acc = model.evaluate(testX_pad, test_Y, verbose = 2, batch_size = 48)\n","print(\"score: %.2f\" % (score))\n","print(\"acc: %.2f\" % (acc))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["score: 0.59\n","acc: 0.80\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5WDjGHqphYBE","colab_type":"text"},"source":["## calculate precision, recall and f1-score"]},{"cell_type":"code","metadata":{"id":"ADGwjq9p4-Nm","colab_type":"code","outputId":"22567b41-eae3-4354-b08a-62d8ff441dc1","executionInfo":{"status":"ok","timestamp":1575573714053,"user_tz":300,"elapsed":81433,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n","test_Y_pred1 = model.predict(testX_pad)\n","test_Y_pred = np.argmax(test_Y_pred1, axis=1)\n","print(classification_report(test_Y, test_Y_pred))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.86      0.90      0.88      1817\n","           1       0.63      0.59      0.61       628\n","           2       0.79      0.68      0.73       483\n","\n","    accuracy                           0.80      2928\n","   macro avg       0.76      0.73      0.74      2928\n","weighted avg       0.80      0.80      0.80      2928\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KMrMeWOOhdEQ","colab_type":"text"},"source":["## calculate confusion matrix"]},{"cell_type":"code","metadata":{"id":"UNtQtjD8GjUo","colab_type":"code","outputId":"fd53a3ce-cdc5-4fdc-bbec-1ebe7a05db69","executionInfo":{"status":"ok","timestamp":1575573714054,"user_tz":300,"elapsed":81393,"user":{"displayName":"Liu Clover","photoUrl":"","userId":"01707038003287610841"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["print(pd.crosstab(test_Y.ravel(), test_Y_pred, rownames = ['True'], colnames = ['Predicted'], margins = True))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Predicted     0    1    2   All\n","True                           \n","0          1641  146   30  1817\n","1           197  371   60   628\n","2            80   74  329   483\n","All        1918  591  419  2928\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7U_p6TU4tK4y","colab_type":"text"},"source":["## Store predictions"]},{"cell_type":"code","metadata":{"id":"Obt6KNkptLX5","colab_type":"code","colab":{}},"source":["df_result = test_X.copy()\n","df_result['prediction'] = test_Y_pred.tolist() \n","\n","#file_name = 'LSTM_prediction'\n","df_result.to_csv(\"/content/drive/My Drive/LSTM_prediction.csv\") # + file_name + '.csv')    "],"execution_count":0,"outputs":[]}]}